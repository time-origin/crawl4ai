## 实施阶段 Day 02 - Reddit 爬虫策略与技术实现 (v1.5)

### 1. 背景与目标

为配合游戏 `Ratio'd` 的内容源，我们需要建立一套能够**准实时发现、智能筛选并深度挖掘** Reddit 政治梗和热点事件的爬虫系统。根据最新会议纪要，我们决定**放弃使用官方API (PRAW) **，以规避复杂的审核和 Key 限制，转而采用更直接的网页抓取技术。

**本日核心目标：**
1.  **策略确立：** 采用“`.json` 接口组合拳”策略，完全不依赖任何 API Key。
2.  **技术实现：** 基于 `requests` 库实现对 Reddit 隐藏 `.json` 接口的访问和解析。
3.  **风险规避：** 重点处理伪装 User-Agent 和应对 IP 频率限制的问题。

### 2. 技术栈与核心策略

#### 2.1. 核心工具

-   **主工具：`requests` 库**
    -   **理由：** 轻量、高效，足以满足对 `.json` 接口的 HTTP 请求需求。
-   **辅助库：无**
    -   由于直接返回 JSON 数据，我们不再需要 `BeautifulSoup` (HTML解析) 或 `PRAW` (API封装)。

#### 2.2. 核心策略：“.json 接口组合拳”

我们将通过在 Reddit URL 后附加 `.json` 后缀来直接获取结构化数据。

1.  **横向监控 (发现“瓜”)**
    -   **方式：** 高频轮询目标板块的 `rising.json` 接口 (例如: `https://www.reddit.com/r/politics/rising.json`)。
    -   **数据源：** 返回的 JSON 数据中，`data.children` 数组包含了帖子的列表，每个帖子都含有标题、分数、评论数等初步信息。

2.  **纵向深挖 (提取“梗”)**
    -   **方式：** 一旦发现潜力贴（如评论数激增），则获取其 `permalink`，并构造该帖子详情页的 `.json` 链接 (例如: `https://www.reddit.com/{permalink}.json`)。
    -   **数据源：** 返回的 JSON 数据是一个包含两个元素的列表：
        -   第一个元素 (`data[0]`) 包含帖子的详细信息。
        -   第二个元素 (`data[1]`) 包含评论树 (`data.children`)。我们可以直接遍历这个树来找到高赞评论。

#### 2.3. 关键技术点与风险规避

-   **伪装 User-Agent (必须)**
    -   所有 `requests` 请求都必须携带一个伪装成主流浏览器的 `User-Agent` 头，否则会立即被 Reddit 以 `429 Too Many Requests` 拒绝。
-   **IP 频率限制**
    -   无 Key 抓取的频率完全基于 IP。为避免被封，请求频率应保持温和（如每分钟不超过10-20次）。
    -   **警告：** 在云服务器（AWS、阿里云等）上运行时，机房 IP 极有可能被直接屏蔽。开发和测试建议在本地或使用住宅代理 (Residential Proxy) 进行。

### 3. 代码架构

由于不再需要复杂的 PRAW 客户端，代码结构可以适当简化。

```
crawl4ai/
└── crawlers/
    ├── _documentation/
    │   └── 实施阶段day-02-1.0.md  # (本计划文档)
    └── reddit/
        ├── __init__.py
        ├── config.py           # 存放所有配置，如板块列表、关键词、User-Agent
        ├── utils.py            # 存放通用的网络请求函数，如 get_reddit_json
        ├── parser.py           # 负责解析 .json 数据，提取帖子和评论
        ├── main.py             # 主入口，编排和调度爬虫流程
        └── models.py           # (可选) 定义数据模型
```
-   `config.py`: 不再需要 API 凭证，但需要保存 User-Agent 和板块列表。
-   `utils.py`: 封装一个通用的 `get_reddit_json` 函数，负责处理请求、Headsers 和基本的错误处理。
-   `parser.py`: 核心逻辑，负责解析从 `.json` 接口获取的复杂 JSON 结构，并转换为我们定义的 `ExtractedPost` 模型。
-   `main.py`: 流程不变，但调用方式从 `monitor`+`extractor` 变为 `utils`+`parser`。

### 4. 功能规划：目标板块与关键词

(此部分与 v1.4 保持不变，但配置将移至新的 `config.py` 中)

#### 4.1. 目标板块 (Subreddits) 清单

此配置将存放在 `config.py` 中。
-   `r/nottheonion`, `r/LeopardsAteMyFace`, `r/PoliticalHumor`, `r/facepalm`, `r/SelfAwarewolves`, `r/OSINT`, `r/intelligence`, `r/NonCredibleDefense`, `r/conspiracy`, `r/ConflictNews`, `r/worldnews`, `r/anime_titties`, `r/politics`, `r/OutOfTheLoop`, `r/PublicFreakout`, `r/ukpolitics`, `r/korea`, `r/europe`, `r/China_irl`, `r/Taiwan`, `r/WhitePeopleTwitter`, `r/BlackPeopleTwitter`, `r/TikTokCringe`

#### 4.2. 关键词“清洗”列表

此配置将存放在 `config.py` 中。
-   **触发词:** `Leak`, `Resign`, `Mistake`, `Sorry`, `Apology`, `Scandal`, `Hot mic`, `Caught`
-   **黑名单词:** `Trump`, `Opinion`, `Megathread`

### 5. 下一步开发计划

1.  **重构文件结构：** 删除 `client.py`, `monitor.py`, `extractor.py`。创建 `utils.py` 和 `parser.py`。
2.  **更新配置 (`config.py`)：** 移除 API 凭证，添加 User-Agent 配置。
3.  **实现网络工具 (`utils.py`)：** 编写 `get_reddit_json` 函数。
4.  **实现解析器 (`parser.py`)：** 编写函数来解析帖子列表和帖子详情的 JSON 数据。
5.  **重构主流程 (`main.py`)：** 修改 `main.py` 以适应新的 `utils` 和 `parser` 模块。
