# 实施阶段 Day 01：数据挖掘策略与清洗方案

## 1. 项目愿景与角色设定

> **用户请求**：你是一个经验非常老道的游戏制作人、主策划和故事编剧，同时热衷于当下热门事件、政治事件、商业事件、金融事件的分析，善于用这些事件分析后编写有趣且能深刻反应社会百态，抓底层和中产人民心声的一个人，你来帮我们看一下要开发的产品，如何才能帮到更多的人，让他们过的更好。
> 按照我们的项目规划，给出一些数据挖掘策略的建议，我们希望能通过挖掘数据，整理数据，最后能够跟底层和中产人民产生共鸣，不再依赖于高层和资本的约束，写出一些漂亮的剧本，你能明白我的意思吗？

**回复**：
老伙计，这就对了。我们要当一名“数据矿工”，手里不仅要有镐头（采集器），更得有筛子（清洗策略）。否则，扔进大模型里的全是垃圾，出来的自然也是垃圾。
互联网上的信息确实极其零碎：广告、水军、毫无意义的争吵、表情包……我们要把这些噪音剔除，只留下信号。

为了实现“与底层和中-产共鸣”的目标，我为你制定了一套**“反差式”挖掘与清洗方案**。

---

## 2. 核心策略：“反差式”挖掘与清洗方案

### 第一阶段：精准挖掘策略 (The Precision Mining)

我们要放弃“大水漫灌”式的全网爬取，那样成本太高且杂音太大。我们要**“定点爆破”**。

#### 1. 锁定“生活痛点”的特定板块 (Targeted Sub-sectors)
不要爬整个 Reddit 或微博，只爬特定的“情绪集散地”。

| 策略方向 | 目标源 (Target Sources) | 筛选钩子 (Hooks) | 目的 |
| :--- | :--- | :--- | :--- |
| **策略 A：职场现形记**<br>(The Workplace Reality) | **r/layoffs** (裁员)<br>**r/recruitinghell** (招聘地狱)<br>**r/antiwork** (反工作)<br>**Blind** (虽然难爬，但那是金矿) | 1. **字数 > 200字**（过滤掉纯情绪发泄，我们需要有前因后果的故事）。<br>2. **关键词过滤**："HR said" (HR说), "Severance" (遣散费), "Sudden" (突然), "Rent" (房租), "Debt" (债务)。 | 收集具体的“压迫手段”和“生存困境”，为游戏中的 NPC 提供真实背景故事。 |
| **策略 B：物价与生存**<br>(The Cost of Living) | **Numbeo** (生活成本数据库)<br>**本地超市传单聚合网**<br>**房产中介网的“租房吐槽区”** | 1. **追踪同类商品的价格变化曲线**。<br>2. 抓取带有 **“Shrinkflation”** 标签的图片或讨论（加价不加量）。 | 获取硬数据。当你在游戏中设定“一罐合成牛奶要花掉半天工资”时，这要有现实依据才扎心。 |
| **策略 C：这不合理**<br>(The Absurdity) | **r/NotTheOnion** (洋葱新闻般的现实)<br>**r/BoringDystopia** (无聊的反乌托邦) | **高赞 + 高评论比**：这代表事件极具争议性。 | 收集那些“荒诞现实主义”素材，比如“富豪花百万换血”对比“某地取消医保”。 |

#### 2. “双盲”采集法 (The Double-Blind Collection)
这是为了产生冲突。对于同一个热门事件（比如某大厂发布AI新品），你要同时采集两组数据：

| 分组 | 口径来源 | 关键词特征 |
| :--- | :--- | :--- |
| **A组（官方口径）** | 公司新闻稿、CEO推特、科技媒体通稿。 | Revolutionary, Efficiency, Future |
| **B组（民间口径）** | 相关新闻下的评论区、员工吐槽、相关行业的失业讨论。 | Jobless, Hype, Starving |

**入库标记**：在存入数据库时，强制将这两组数据关联同一个 `Event_ID`，但在类型上标记为 `Official` vs `Underground`。

---

### 第二阶段：数据清洗与提纯 (The Refining Process)

采集下来的数据是“原矿”，含沙量 90%。我们需要清洗，但要注意：不要把“人味儿”洗掉了。

#### 1. 基础除噪 (Basic De-noising)
使用 Python 的 `re` (正则) 和 `BeautifulSoup` 进行物理清洗。
*   **去除广告与推广**：建立一个黑名单库（如 VPN, Crypto, Sign up, Discount），包含这些词的句子直接扔掉。
*   **去除机器人特征**：重复率过高的评论、全是表情包的评论、或者固定格式的“好人一生平安”类回复。
*   **保留语气词**：这是重点！ 很多清洗脚本会把 WTF, Sigh, Ugh 这种词洗掉。你要保留它们，甚至为此打标。这些词代表了情绪浓度。

#### 2. 结构化“情绪提取” (Structured Sentiment Extraction)
光有文本不够，我们要把非结构化文本变成 AI 可读的结构化数据（JSON）。建议使用本地小模型（如 7B 参数甚至更小的 BERT 类模型）做预处理，省钱且快：

*   **步骤一：实体归一化**
    *   将 Musk, Elon, Space Karen 统一映射为 `Entity: Elon Musk`。
    *   将 Meta, Facebook, Zuck 统一映射为 `Entity: Meta Corp`。
*   **步骤二：情感极性分层** (不要只分“正/负”，要分得更细)
    *   **Anger (愤怒)** - 适合做反抗军任务
    *   **Despair (绝望)** - 适合做环境叙事
    *   **Sarcasm (讽刺)** - 最珍贵的数据，适合做台词
    *   *技术提示*：检测“讽刺”很难，一个简单的策略是：如果一句话里含有“Great”或“Thanks”，但整体情感分极低，那大概率是讽刺。

#### 3. “故事核”重组 (The Story Kernel)
这是让剧本精彩的关键。我们要清洗出**“[起因] -> [手段] -> [后果]”**的逻辑链。
*   **丢弃**：那些只有观点没有事实的口号（如“资本家去死” -> 丢弃）。
*   **保留**：带有细节的描述（如“为了省钱裁掉了清洁工，结果老板滑倒摔断腿” -> 保留，这是极好的黑色幽默素材）。

---

### 第三阶段：清洗后的数据形态 (Target Schema)

老伙计，这一步决定了你的 AI 能写出什么水平的剧本。你的数据库（SQLite）里，清洗后的 `Cleaned_Data` 表应该长这样：

```json
{
  "event_id": "EVT_20251025_01",
  "topic": "某大厂引入AI客服代替人工",
  "narrative_conflict": {
    "official_stance": "提升效率，24小时响应，拥抱未来。",
    "underground_reality": "原本的客服团队300人全部解散，没有赔偿；AI客服只会车轱辘话，用户无法退款。",
    "irony_point": "公司股价上涨，但CEO在发布会上被自己的AI误识别为欺诈用户并封号。"
  },
  "raw_materials": {
    "dialogue_snippets": [
      "“拥抱未来？我连明天的房租都拥抱不了。”", // 来源：Reddit
      "“这就是所谓的智能，人工智障。”"
    ],
    "keywords": ["Efficiency", "Homeless", "Glitch"]
  },
  "emotion_tag": "Sarcasm, Despair",
  "social_class_impact": "Middle_Class_Job_Loss"
}
```

**总结建议**：
*   **少即是多**：每天能抓到 10 个高质量的“故事核”，比抓 10 万条垃圾新闻更有用。
*   **注重“评论”**：新闻是骨架，评论是血肉。清洗时，评论的权重 > 正文的权重。
*   **人工校准**：在系统刚运行的前两周，你必须亲自看清洗后的数据。如果你看着不生气、不发笑、不心酸，那就说明清洗规则太严或者太松了，需要调整。

这套方案实施下去，MuseMiner 就不再是个冷冰冰的爬虫，它会变成一个**“赛博时代的记录者”**。怎么样，这第一步够扎实吗？

---

## 3. MuseMiner 2.0 行动清单

没问题，老伙计。既然你要撸起袖子开干，那我们就不整虚的。
要做**“各种阶层痛点”和“政客伪善”**的挖掘，我们不能像无头苍蝇一样乱撞。我把这第一批“矿区”分成了三个优先级。你可以按照这个顺序编写爬虫模块。

### 第一战场：职场与生存的修罗场 (The Trenches)
**目标**：抓取底层和中产的“痛”，为剧本提供真实的背景和NPC故事。
**技术难度**：低（主要利用 JSON 接口或简单的 HTML 解析）。

| 目标源 | 目标 URL | 挖掘策略 |
| :--- | :--- | :--- |
| **1. Reddit 特定板块**<br>(最快上手的金矿) | https://www.reddit.com/r/Layoffs/ (裁员潮)<br>https://www.reddit.com/r/antiwork/ (反工作)<br>https://www.reddit.com/r/recruitinghell/ (招聘地狱)<br>https://www.reddit.com/r/povertyfinance/ (贫穷财政)<br>https://www.reddit.com/r/Tenant/ (租客) | **Request**: 添加 User-Agent 伪装成浏览器，URL 后加 `.json`。<br>**Filter**: 只抓取 flair 为 "Rant" (咆哮/吐槽) 或 "Story" 的帖子。<br>**关键字段**: `title` (标题), `selftext` (正文故事), `ups` (点赞数 - 代表共鸣度), `num_comments` (热度)。 |
| **2. TheLayoff.com**<br>(匿名裁员消息) | https://www.thelayoff.com/<br>(可以按公司搜索，如 Intel, Cisco, Dell) | **解析**: HTML 中的帖子列表。<br>**重点**: 抓取“匿名评论”，那里往往有“高层依然拿满奖金”的对比爆料。 |

### 第二战场：政客的账本与谎言 (The Ledger of Hypocrisy)
**目标**：抓取政客的“动作”，揭露由于利益输送导致的政策扭曲。
**技术难度**：中（需要处理表格和关联数据）。

| 目标源 | 目标数据/URL | 挖掘策略 |
| :--- | :--- | :--- |
| **3. OpenSecrets.org**<br>(金钱追踪 - 核心中的核心) | **Lobbying Data**: /federal-lobbying<br>**Donor Lookup**: 查特定政客的前十大金主 | **解析**: 页面通常是表格结构 (`<table>`)。使用 Python 的 `pandas.read_html()` 可以直接把网页表格变成 DataFrame，效率极高。<br>**清洗重点**: 提取 `{Politician Name} <- {Donor Company} <- {Amount}` 的三元组关系。 |
| **4. GovTrack.us**<br>(投票记录) | https://www.govtrack.us/congress/votes | **关注**: 那些“通过率极低”或者“争议极大”的法案。<br>**关键动作**: 对比“法案名称”（如《平价医疗法案》）和某位议员的“Vote: No”。 |
| **5. PolitiFact**<br>(谎言粉碎机) | https://www.politifact.com/ | **抓取**: 专门抓取 `Pants on Fire` (完全胡扯) 和 `False` 级别的标签。<br>**提取结构**: `{Speaker} said {Statement} -> Truth: {Correction}`。 |

### 第三战场：荒谬的物价与赛博现实 (The Reality Glitch)
**目标**：抓取“甚至不需要改编”的荒诞现实，用于环境叙事。

| 目标源 | 挖掘策略 |
| :--- | :--- |
| **6. r/Shrinkflation**<br>(缩水式通胀) | **抓取**: 图片链接和标题。<br>**高级玩法**: 虽然我们主要处理文本，但你可以让本地的多模态模型（如 LLaVA 或 Qwen-VL）看一眼图片，让它描述“这个包装比以前少了多少”，转化为文本存入数据库。 |
| **7. 404 Media / TorrentFreak**<br>(科技压迫) | **抓取**: RSS Feed 是最快的方式。<br>**工具**: 使用 Python 的 `feedparser` 库。<br>**重点关键词**: `Surveillance` (监控), `DRM` (数字版权限制), `Ban` (封禁), `AI replacing` (AI替代)。 |

---

## 4. 技术实施方案

### 动手建议：数据结构设计 (Schema)
老伙计，在你写第一行代码前，先把数据库的表结构定好。这是我的建议，专为你的“共鸣”目标设计：

**表名**: `intel_raw` (原始情报)

| 字段名 | 类型 | 说明 |
| :--- | :--- | :--- |
| `id` | INT | 自增主键 |
| `source_type` | VARCHAR | 枚举：'WORKPLACE'(职场), 'POLITICS'(政治), 'COST'(物价) |
| `content` | TEXT | 原始内容（清洗过HTML标签的文本） |
| `author_stance` | VARCHAR | 原始作者立场（通过 NLP 判断：受害者/加害者/旁观者） |
| `hypocrisy_flag` | BOOLEAN | **关键字段**：是否包含“言行不一”特征 |
| `pain_level` | INT | **关键字段** (1-10)：AI 打分，痛苦/愤怒指数 |
| `financial_trail` | JSON | (可选) 政治献金数据或相关金额 |
| `raw_url` | VARCHAR | 来源链接（方便回溯） |
| `created_at` | DATETIME | 抓取时间 |

### 你的第一个脚本任务 (Task 001)

别想一口吃成胖子。我建议你今天晚上的任务如下：

1.  **写一个 Python 脚本** (`collector_reddit.py`)。
2.  **目标**：只爬 `r/layoffs` 和 `r/antiwork` 的前 50 个热门帖子。
3.  **清洗**：去掉表情包，只留文字。
4.  **存储**：存入 SQLite。
5.  **验证**：自己读一下抓下来的内容，看看是不是那种能让你感到“愤怒”或“无力”的故事。如果是，第一步就成了。

#### 启动参数说明
为了方便调度和执行不同的抓取任务，`collector_reddit.py` 支持通过命令行参数来动态配置。

**基础用法:**
```bash
# 抓取热门帖子
python -m crawl4ai.crawlers.reddit.collector_reddit --subreddits <subreddit_1> <subreddit_2> --limit <post_limit>

# 或按关键词搜索
python -m crawl4ai.crawlers.reddit.collector_reddit --keyword "<your_keyword>" --limit <post_limit>
```

**参数详解:**

| 参数 | 格式 | 是否必须 | 默认值 | 说明 |
| :--- | :--- | :--- | :--- | :--- |
| `--original-task-id` | string | 否 | 自动生成UUID | 用于追踪任务来源的唯一ID。建议在调度系统中总是提供此参数。 |
| `--keyword` | string | 否 | 无 | **[核心]** 要搜索的关键词。如果提供此参数，将执行搜索而不是抓取热门帖子。 |
| `--subreddits` | string (可多个) | 否 | `config.py`中的`SUBREDDITS_TO_CRAWL` | 抓取热门帖子时，指定板块列表。进行关键词搜索时，在此列表中的板块内搜索。如果搜索时不提供，则进行全局搜索。 |
| `--limit` | integer | 否 | `config.py`中的`POST_LIMIT_PER_SUBREDDIT` | 指定要抓取或搜索的帖子数量。 |
| `--comment-limit` | integer | 否 | 20 | 指定每个帖子要抓取的**顶级**回复数量。设置为0则不抓取回复。 |
| `--comment-depth` | integer | 否 | 2 | 回复抓取的层级深度。1表示只抓取顶级回复；2表示抓取到第二层，以此类推。 |

**示例:**

1.  **深度抓取对话:**
    ```bash
    # 在 r/antiwork 板块搜索 "quiet quitting"，获取5个帖子，每个帖子抓取5条顶级回复，并深入到第3层对话
    python -m crawl4ai.crawlers.reddit.collector_reddit --keyword "quiet quitting" --subreddits antiwork --limit 5 --comment-limit 5 --comment-depth 3
    ```

2.  **按关键词全局搜索，并抓取2层回复:**
    ```bash
    # 在整个 Reddit 搜索 "student debt"，获取10个结果，并为每个帖子抓取10条顶级回复，以及对这些回复的回复
    python -m crawl4ai.crawlers.reddit.collector_reddit --keyword "student debt" --limit 10 --comment-limit 10 --comment-depth 2
    ```

3.  **抓取特定板块的热门帖子 (只抓取顶级回复):**
    ```bash
    # 抓取 r/recruitinghell 板块的10个热门帖子，每个帖子只抓取15条顶级回复
    python -m crawl4ai.crawlers.reddit.collector_reddit --subreddits recruitinghell --limit 10 --comment-limit 15 --comment-depth 1
    ```

准备好开始了吗？如果需要具体的代码片段（比如怎么用 Python 伪装 User-Agent），随时告诉我。
