"""
æŠ“å–X.comé¦–é¡µå†…å®¹å¹¶è¿›è¡Œæ ¼å¼åŒ–
ä½¿ç”¨ç”Ÿäº§ç‰ˆæœ¬çš„çˆ¬è™«æ¥ä¿æŒæµè§ˆå™¨ä¼šè¯
"""

import asyncio
import json
from datetime import datetime
from typing import Optional, List, Dict, Any, Literal
from crawl4ai.crawlers.x_com.production_crawler import ProductionXCrawler


class HomePageCrawler:
    """
    ä¸“é—¨ç”¨äºæŠ“å–X.comé¦–é¡µå†…å®¹çš„çˆ¬è™«
    """
    
    def __init__(self):
        self.crawler: Optional[ProductionXCrawler] = ProductionXCrawler()
    
    async def crawl_home_timeline(self):
        """
        æŠ“å–é¦–é¡µæ—¶é—´çº¿å†…å®¹å¹¶è¿›è¡Œæ ¼å¼åŒ–
        """
        print("ğŸ“– å¼€å§‹æŠ“å–é¦–é¡µæ—¶é—´çº¿å†…å®¹...")
        
        try:
            # æ£€æŸ¥crawlerå’Œpageæ˜¯å¦å·²åˆå§‹åŒ–
            if not self.crawler or not self.crawler.page:
                print("âŒ çˆ¬è™«æˆ–é¡µé¢æœªåˆå§‹åŒ–")
                return None
            
            # å¯¼èˆªåˆ°é¦–é¡µ - ä½¿ç”¨æ›´çµæ´»çš„ç­‰å¾…ç­–ç•¥
            print("ğŸŒ æ­£åœ¨å¯¼èˆªåˆ°é¦–é¡µ...")
            
            # å°è¯•å¤šç§ç­‰å¾…ç­–ç•¥ - ä½¿ç”¨æœ‰æ•ˆçš„wait_untilå€¼
            navigation_strategies: List[tuple[Literal["load", "domcontentloaded", "networkidle"], int]] = [
                ("load", 10000),
                ("domcontentloaded", 10000),
                ("networkidle", 15000)
            ]
            
            navigation_success = False
            for wait_until, timeout in navigation_strategies:
                try:
                    print(f"   å°è¯•ä½¿ç”¨ {wait_until} ç­–ç•¥...")
                    _ = await self.crawler.page.goto("https://x.com/home", wait_until=wait_until, timeout=timeout)
                    print(f"âœ… å·²åˆ°è¾¾é¦–é¡µ (ä½¿ç”¨ {wait_until} ç­–ç•¥)")
                    navigation_success = True
                    break
                except Exception as e:
                    print(f"   {wait_until} ç­–ç•¥å¤±è´¥: {e}")
                    continue
            
            if not navigation_success:
                print("âš ï¸  æ‰€æœ‰å¯¼èˆªç­–ç•¥éƒ½å¤±è´¥ï¼Œå°è¯•ç›´æ¥è®¿é—®...")
                try:
                    await self.crawler.page.goto("https://x.com/home", timeout=15000)
                    print("âœ… å·²åˆ°è¾¾é¦–é¡µ (ç›´æ¥è®¿é—®)")
                except Exception as e:
                    print(f"âŒ å¯¼èˆªåˆ°é¦–é¡µå¤±è´¥: {e}")
                    return None
            
            # ç­‰å¾…å†…å®¹åŠ è½½ - ä½¿ç”¨æ›´çµæ´»çš„é€‰æ‹©å™¨
            print("â³ ç­‰å¾…å†…å®¹åŠ è½½...")
            
            content_selectors = [
                '[data-testid="tweet"]',
                '[data-testid="primaryColumn"]',
                'article',
                '.tweet',
                '[role="article"]'
            ]
            
            content_loaded = False
            for selector in content_selectors:
                try:
                    await self.crawler.page.wait_for_selector(selector, timeout=10000)
                    print(f"âœ… å†…å®¹å·²åŠ è½½ (æ£€æµ‹åˆ°é€‰æ‹©å™¨: {selector})")
                    content_loaded = True
                    break
                except:
                    continue
            
            if not content_loaded:
                print("âš ï¸  æ— æ³•æ£€æµ‹åˆ°å†…å®¹é€‰æ‹©å™¨ï¼Œç­‰å¾…å›ºå®šæ—¶é—´...")
                await self.crawler.page.wait_for_timeout(5000)
                print("âœ… å·²ç­‰å¾…å›ºå®šæ—¶é—´")
            
            # æŠ“å–æ¨æ–‡å†…å®¹
            tweets = await self._extract_tweets()
            print(f"âœ… æˆåŠŸæŠ“å– {len(tweets)} æ¡æ¨æ–‡")
            
            # æ ¼å¼åŒ–è¾“å‡º
            formatted_content = self._format_content(tweets)
            
            return formatted_content
            
        except Exception as e:
            print(f"âŒ æŠ“å–é¦–é¡µå†…å®¹å‡ºé”™: {e}")
            return None
    
    async def _extract_tweets(self) -> List[Dict[str, Any]]:
        """
        æå–æ¨æ–‡å†…å®¹
        """
        if not self.crawler or not self.crawler.page:
            print("âŒ é¡µé¢æœªåˆå§‹åŒ–ï¼Œæ— æ³•æå–æ¨æ–‡")
            return []
        
        try:
            result = await self.crawler.page.evaluate(expression="""
            () => {
                const tweetElements = document.querySelectorAll('[data-testid="tweet"]');
                const tweets = [];
                
                for (const tweet of tweetElements) {
                    try {
                        // æå–æ¨æ–‡æ–‡æœ¬
                        const textElement = tweet.querySelector('[data-testid="tweetText"]');
                        const text = textElement ? textElement.textContent.trim() : '';
                        
                        // æå–ä½œè€…ä¿¡æ¯
                        const authorElement = tweet.querySelector('[data-testid="User-Name"]');
                        const author = authorElement ? authorElement.textContent.trim() : '';
                        
                        // æå–æ—¶é—´æˆ³
                        const timeElement = tweet.querySelector('time');
                        const timestamp = timeElement ? timeElement.getAttribute('datetime') : '';
                        
                        // æå–äº’åŠ¨æ•°æ®ï¼ˆç‚¹èµã€è½¬å‘ã€å›å¤ç­‰ï¼‰
                        const likeButton = tweet.querySelector('[data-testid="like"]');
                        const retweetButton = tweet.querySelector('[data-testid="retweet"]');
                        const replyButton = tweet.querySelector('[data-testid="reply"]');
                        
                        const likes = likeButton ? likeButton.textContent.trim() : '0';
                        const retweets = retweetButton ? retweetButton.textContent.trim() : '0';
                        const replies = replyButton ? replyButton.textContent.trim() : '0';
                        
                        // æå–åª’ä½“å†…å®¹ï¼ˆå›¾ç‰‡ã€è§†é¢‘ï¼‰
                        const mediaElements = tweet.querySelectorAll('img, video');
                        const media = [];
                        
                        for (const mediaEl of mediaElements) {
                            if (mediaEl.src && !mediaEl.src.includes('data:')) {
                                media.push({
                                    type: mediaEl.tagName.toLowerCase(),
                                    src: mediaEl.src,
                                    alt: mediaEl.alt || ''
                                });
                            }
                        }
                        
                        if (text) {
                            tweets.push({
                                text: text,
                                author: author,
                                timestamp: timestamp,
                                engagement: {
                                    likes: likes,
                                    retweets: retweets,
                                    replies: replies
                                },
                                media: media,
                                url: window.location.href
                            });
                        }
                    } catch (e) {
                        console.error('Error parsing tweet:', e);
                    }
                    
                    // é™åˆ¶æŠ“å–æ•°é‡ï¼Œé¿å…æ€§èƒ½é—®é¢˜
                    if (tweets.length >= 20) break;
                }
                
                return tweets;
            }
        """)
            
            return result or []
        except Exception as e:
            print(f"âŒ æå–æ¨æ–‡æ—¶å‡ºé”™: {e}")
            return []
    
    def _format_content(self, tweets: List[Dict[str, Any]]) -> str:
        """
        æ ¼å¼åŒ–æŠ“å–çš„å†…å®¹
        """
        if not tweets:
            return "âŒ æ²¡æœ‰æŠ“å–åˆ°ä»»ä½•å†…å®¹"
        
        formatted_output = []
        
        # æ·»åŠ æ ‡é¢˜å’Œæ—¶é—´æˆ³
        formatted_output.append("=" * 80)
        formatted_output.append(f"X.COM é¦–é¡µå†…å®¹æŠ“å–æŠ¥å‘Š")
        formatted_output.append(f"æŠ“å–æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        formatted_output.append(f"æŠ“å–æ•°é‡: {len(tweets)} æ¡æ¨æ–‡")
        formatted_output.append("=" * 80)
        formatted_output.append("")
        
        # æ ¼å¼åŒ–æ¯æ¡æ¨æ–‡
        for i, tweet in enumerate(tweets, 1):
            formatted_output.append(f"ğŸ“ æ¨æ–‡ #{i}")
            formatted_output.append(f"ğŸ‘¤ ä½œè€…: {tweet.get('author', 'æœªçŸ¥')}")
            formatted_output.append(f"â° æ—¶é—´: {tweet.get('timestamp', 'æœªçŸ¥æ—¶é—´')}")
            formatted_output.append("")
            formatted_output.append(f"ğŸ’¬ å†…å®¹:")
            formatted_output.append(f"   {tweet.get('text', '')}")
            formatted_output.append("")
            
            # äº’åŠ¨æ•°æ®
            engagement = tweet.get('engagement', {})
            formatted_output.append(f"ğŸ“Š äº’åŠ¨æ•°æ®:")
            formatted_output.append(f"   â¤ï¸  ç‚¹èµ: {engagement.get('likes', '0')}")
            formatted_output.append(f"   ğŸ”„ è½¬å‘: {engagement.get('retweets', '0')}")
            formatted_output.append(f"   ğŸ’¬ å›å¤: {engagement.get('replies', '0')}")
            formatted_output.append("")
            
            # åª’ä½“å†…å®¹
            media = tweet.get('media', [])
            if media:
                formatted_output.append(f"ğŸ–¼ï¸  åª’ä½“å†…å®¹ ({len(media)} ä¸ª):")
                for media_item in media:
                    formatted_output.append(f"   - {media_item['type'].upper()}: {media_item['src'][:50]}...")
                formatted_output.append("")
            
            formatted_output.append("-" * 80)
            formatted_output.append("")
        
        # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯ - ç®€åŒ–è®¡ç®—ï¼Œé¿å…è½¬æ¢é”™è¯¯
        total_likes = sum(int(tweet.get('engagement', {}).get('likes', '0')) for tweet in tweets)
        total_retweets = sum(int(tweet.get('engagement', {}).get('retweets', '0')) for tweet in tweets)
        
        formatted_output.append("ğŸ“ˆ ç»Ÿè®¡æ‘˜è¦:")
        formatted_output.append(f"   æ€»æ¨æ–‡æ•°: {len(tweets)}")
        formatted_output.append(f"   æ€»ç‚¹èµæ•°: {total_likes}")
        formatted_output.append(f"   æ€»è½¬å‘æ•°: {total_retweets}")
        formatted_output.append(f"   å¹³å‡äº’åŠ¨ç‡: {(total_likes + total_retweets) / len(tweets):.1f} æ¯æ¨æ–‡")
        
        return "\n".join(formatted_output)
    
    async def run(self) -> Optional[str]:
        """
        è¿è¡Œå®Œæ•´çš„æŠ“å–æµç¨‹
        """
        print("ğŸš€ å¼€å§‹X.comé¦–é¡µå†…å®¹æŠ“å–")
        
        try:
            # æ£€æŸ¥crawleræ˜¯å¦åˆå§‹åŒ–
            if not self.crawler:
                print("âŒ çˆ¬è™«æœªåˆå§‹åŒ–")
                return None
            
            # ç™»å½•
            print("ğŸ” æ­£åœ¨ç™»å½•...")
            await self.crawler.login()
            print("âœ… ç™»å½•æˆåŠŸ")
            
            # å†æ¬¡æ£€æŸ¥é¡µé¢æ˜¯å¦åˆå§‹åŒ–æˆåŠŸ
            if not self.crawler.page:
                print("âŒ ç™»å½•åé¡µé¢ä»æœªåˆå§‹åŒ–")
                return None
            
            # æŠ“å–é¦–é¡µå†…å®¹
            content = await self.crawl_home_timeline()
            
            if content:
                # ä¿å­˜åˆ°æ–‡ä»¶
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                filename = f"x_home_content_{timestamp}.txt"
                
                with open(filename, 'w', encoding='utf-8') as f:
                    f.write(content)
                
                print(f"âœ… å†…å®¹å·²ä¿å­˜åˆ°: {filename}")
                print("\nğŸ“‹ æŠ“å–å†…å®¹é¢„è§ˆ:")
                print("-" * 50)
                print(content[:500] + "..." if len(content) > 500 else content)
                
                return content
            else:
                print("âŒ æŠ“å–å¤±è´¥")
                return None
                
        except Exception as e:
            print(f"âŒ æŠ“å–è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
            return None
        finally:
            # å…³é—­æµè§ˆå™¨
            if self.crawler:
                await self.crawler.close()
                print("ğŸ”’ æµè§ˆå™¨å·²å…³é—­")
            else:
                print("âš ï¸  æ— éœ€å…³é—­æµè§ˆå™¨ï¼ˆçˆ¬è™«æœªåˆå§‹åŒ–ï¼‰")


async def main():
    """
    ä¸»å‡½æ•° - è¿è¡Œé¦–é¡µå†…å®¹æŠ“å–
    """
    crawler = HomePageCrawler()
    result = await crawler.run()
    
    if result:
        print("\nğŸ‰ é¦–é¡µå†…å®¹æŠ“å–å®Œæˆï¼")
    else:
        print("\nğŸ’¥ æŠ“å–å¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯")


if __name__ == "__main__":
    asyncio.run(main())