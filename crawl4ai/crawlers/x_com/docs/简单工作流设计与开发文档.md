# 简单工作流设计与开发文档

**版本**: 1.1
**日期**: 2025-10-18

## 1. 概述

本文档旨在为 `crawl4ai` (数据抓取服务)、`slm-trainer-rs` (数据处理服务) 和 `slm-fine-tuning-py` (教师推理服务) 的开发者提供一个统一的设计和开发指南，以实现一个简单、安全、高效的异步任务处理工作流。

### 1.1. 核心问题

在当前的分布式系统中，`slm-trainer-rs` 无法准确得知由 `crawl4ai` 发起的、通过 Kafka 传递的数据流何时结束，导致无法在正确的时间点更新用户创建的原始任务的状态。

### 1.2. 解决方案

我们引入一个基于 **Redis 的集中式计数器** 机制。工作流的核心思想是：

1.  **定义“工作单元”**: 一个“工作单元”被定义为一条由爬虫抓取的推文数据。
2.  **爬虫报告总数**: `crawl4ai` 在完成数据抓取后，会预先计算出本次任务包含的**总推文数**，并通过一个特殊的 Kafka 消息 (`TASK_INIT`) 发送给 `slm-trainer-rs`。
3.  **Redis 计数**: `slm-trainer-rs` 收到 `TASK_INIT` 消息后，在 Redis 中为该任务初始化一个计数器。当每处理完一个工作单元（一条推文），就在 Redis 中原子性地增加一个“已处理”计数。
4.  **状态判断**: 当“已处理”的计数值等于预先报告的“总推文数”时，`slm-trainer-rs` 就判定该任务完成，并更新数据库中的任务状态。

### 1.3. 任务完成的定义 (简化)

为了实现简单和安全，我们将一个任务的 `Completed` 状态定义为：

> **所有由爬虫抓取的推文数据，其包含的所有回复，都已经被成功提交给了后端的 gRPC 任务调度器 (`task_scheduler`)。**

这个定义意味着，我们信任 gRPC 调用的提交过程，而不等待其返回结果。任务的 `Failed` 状态则由两种情况触发：爬虫容器启动或执行失败；或 `slm-trainer-rs` 在处理过程中发生不可恢复的错误。

---

## 2. 共享数据结构与变量 (The Contract)

本节为 `crawl4ai` 和 `slm-trainer-rs` 两个服务的开发者提供必须严格遵守的、统一的共享数据定义。

### 2.1. Command-Line Argument

-   **变量名**: `--original-task-id`
-   **Data Type**: `Integer`

#### `slm-trainer-rs` (Caller)
-   **File**: `src/bizs/task/manager.rs`
-   **Action**: 在 `execute_crawler_command` 函数中，将此参数添加到 `docker run` 命令。

#### `crawl4ai` (Receiver)
-   **File**: `crawl4ai/crawlers/x_com/production_crawler.py`
-   **Action**: 使用 `ArgumentParser` 接收此参数。

### 2.2. Kafka 主题 (Topics)

| 主题名称 | 用途 | 消息格式 |
|---|---|---|
| `task_control_topic` | **(新增)** 用于传输任务控制消息，如初始化计数。 | `TaskControlMessage` |
| `x_com_scraped_data` | **(现有)** 用于传输爬虫抓取的原始推文数据。 | `RawScrapedDataMessage` |

### 2.3. Kafka 消息负载 (One-for-One Contract)

#### `task_control_topic` Message (`TaskControlMessage`)

**JSON 结构**: 
```json
{
  "type": "TASK_INIT",
  "original_task_id": 123,
  "total_tweets": 450
}
```

**字段定义**:

| 字段 | JSON 类型 | 描述 |
|---|---|---|
| `type` | `String` | 必须为字面量 `"TASK_INIT"`。 |
| `original_task_id` | `Number` | 来自命令行的原始任务ID。 |
| `total_tweets` | `Number` | 本次任务将发送的推文总数。 |

**`slm-trainer-rs` 消费者侧结构体**:
- **File**: `src/cleaner/payloads.rs` (建议新增)
- **Struct**:
```rust
use serde::Deserialize;

#[derive(Deserialize, Debug, Clone)]
pub struct TaskControlMessage {
    pub r#type: String, // r# 用于处理Rust关键字
    pub original_task_id: i64,
    pub total_tweets: i64,
}
```

#### `x_com_scraped_data` Message (`RawScrapedDataMessage`)

**JSON 结构**:
```json
{
  "original_task_id": 123,
  "tweet_data": {
    "id": "1977372429597954366",
    "full_text": "...",
    "replies": [ ... ]
  }
}
```

**字段定义**:

| 字段 | JSON 类型 | 描述 |
|---|---|---|
| `original_task_id` | `Number` | 来自命令行的原始任务ID。 |
| `tweet_data` | `Object` | 包含单条推文所有详细信息的对象。 |

**`slm-trainer-rs` 消费者侧结构体**:
- **File**: `src/cleaner/payloads.rs`
- **Struct**:
```rust
use serde::Deserialize;

#[derive(Deserialize, Debug, Clone)]
pub struct RawScrapedPayload {
    pub original_task_id: i64,
    pub tweet_data: RawTweetData,
}

#[derive(Deserialize, Debug, Clone)]
pub struct RawTweetData {
    pub id: String,
    pub url: String,
    pub author: String,
    pub full_text: String,
    pub reply_count: String,
    pub repost_count: String,
    pub like_count: String,
    pub bookmark_count: String,
    pub view_count: String,
    #[serde(default)]
    pub replies: Vec<RawReply>,
}

#[derive(Deserialize, Debug, Clone)]
pub struct RawReply {
    pub author: String,
    pub text: String,
}
```

### 2.4. Redis 键结构

-   **Key 格式**: `task_progress:{original_task_id}`
-   **Type**: `Hash`
-   **Hash Fields**:

| 字段 | 类型 | 描述 |
|---|---|---|
| `total_tweets` | `Integer` | 由 `TASK_INIT` 消息设置的预期总推文数。 |
| `processed_tweets`| `Integer` | **(新增)** 已处理的推文数，通过 `HINCRBY` 原子性递增。 |
| `status` | `String` | 任务在 Redis 中的内部状态，例如 `INITIALIZED`, `PROCESSING`, `COMPLETED`。 |

---

## 3. 服务修改详单

### 3.1. `crawl4ai` (数据抓取服务)

**负责人**: Python 开发者

1.  **接收 `original_task_id`**: 在 `production_crawler.py` 的 `ArgumentParser` 中添加 `--original-task-id` (必需, int)。
2.  **收集数据**: 在 `main` 函数中，修改原有的流式处理逻辑，先将所有抓取到的推文详情 (`detailed_data`) 收集到一个列表 `all_detailed_data` 中。
3.  **发送消息**: 在抓取循环结束后，计算 `total_tweets = len(all_detailed_data)`。如果大于0，则：
    a.  调用新的 `send_task_init` 函数，向 `task_control_topic` 发送一条 `TaskControlMessage`。
    b.  调用修改后的 `send_scraped_data` 函数，遍历 `all_detailed_data`，向 `x_com_scraped_data` 发送所有 `RawScrapedDataMessage`。
4.  **更新 `kafka_manager.py`**: 
    a.  实现 `send_task_init` 函数。
    b.  将原 `send_to_kafka` 重命名为 `send_scraped_data` 并更新其逻辑。

### 3.2. `slm-trainer-rs` (数据处理服务)

**负责人**: Rust 开发者

1.  **新增 Kafka 消费者**: 在 `src/cleaner/mod.rs` 中，为 `task_control_topic` 创建一个新的消费者。收到消息后，调用 `redis_manager.init_task_progress` 初始化 Redis 中的 Hash。
2.  **修改 `processing.rs`**: 在 `process_raw_tweet` 函数成功处理完一条推文（即所有 `replies` 都已提交给 `task_scheduler`）后，调用 `redis_manager.increment_and_check_completion`。
3.  **检查完成状态**: 如果 `increment_and_check_completion` 返回 `true`，则调用 `task::update_task_status` 将数据库中的任务状态更新为 `Completed`。
4.  **实现 `RedisManager`**: 在 `src/middleware/redis_manager.rs` 中实现 `init_task_progress` 和 `increment_and_check_completion` 方法。后者必须使用 Lua 脚本来保证原子性。
5.  **调整 `task_manager.rs`**: 移除 `task_handler` 中当爬虫成功退出时将任务状态更新为 `Completed` 的逻辑。

### 3.3. `slm-fine-tuning-py` (教师推理服务)

**负责人**: Python 开发者

**修改点**: **无**。在此方案下，该服务无需任何修改。

---

## 4. 总结

此方案通过引入一个简单的、集中的计数服务，解耦了数据抓取和数据处理两个阶段的状态判断，同时避免了对外部推理服务回调的依赖，从而实现了一个更简单、更健壮、更易于调试的异步任务工作流。