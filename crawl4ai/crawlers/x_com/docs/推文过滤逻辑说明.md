### **最终方案：基于重试与数据一致性的稳健过滤逻辑**

**核心思想:**
为解决因网络波动、反爬虫策略导致的抓取失败问题，并确保数据质量与一致性，我们采用一个包含 **加长等待、可配置重试、确保数据一致性** 的三位一体最终方案。

**方案优点:**
1.  **高成功率:** 将等待回复出现的超时时间延长至20秒，为页面加载提供更充足的缓冲。
2.  **高鲁棒性:** 引入了可配置的重试机制。当首次抓取回复失败时，程序会自动进行额外尝试，有效对抗偶发性网络错误。
3.  **高数据质量:** 始终以**实际抓取到的、包含文本的回复列表 (`replies`)** 作为判断推文有效性的唯一标准。
4.  **高一致性:** 最终输出到 Kafka 的 `reply_count` 字段值，被强制设为实际抓取到的回复列表的长度，彻底杜绝了元数据与实际数据不一致的问题。
5.  **高灵活性:** 所有关键参数（如重试次数）均可通过 `.env` 文件配置，无需重新编译镜像。

---

**修改文件清单:**
1.  `crawl4ai/crawlers/x_com/config.py`
2.  `crawl4ai/crawlers/x_com/scenes/tweet_detail_scene.py`
3.  `crawl4ai/crawlers/x_com/production_crawler.py`

---

**重构后的执行流程:**

**1. 配置 (`config.py`):**
*   **[修正]** `MAX_RETRIES_ON_FAILURE` 现在通过 `os.getenv("MAX_RETRIES_ON_FAILURE", 1)` 从环境变量获取，使其可在运行时动态配置。

**2. 场景层 (`tweet_detail_scene.py`):**
*   **延长等待:** 将等待第一条回复出现的超时时间增加到 **20秒**。
*   **确保一致性:** 在返回数据时，将 `reply_count` 字段的值强制设置为 `str(len(replies))`，确保其准确反映抓取到的回复数量。

**3. 主逻辑层 (`production_crawler.py`):**
*   **步骤 2/5 (抓取详情):** 此步骤被重构，以包含一个完整的重试循环。
    *   程序会读取 `config.MAX_RETRIES_ON_FAILURE` 的值。
    *   对于每个URL，如果首次抓取未能获得任何回复，程序会根据配置的次数进行循环重试。
    *   清晰的日志会记录每一次尝试的成功或失败。
*   **步骤 3/5 (过滤):** 过滤逻辑保持不变，依然是检查 `len(tweet.get("replies", [])) > 0`，因为它现在处理的是经过重试后、最可靠的数据。

**代码结构示例 (伪代码):**

```python
# config.py
MAX_RETRIES_ON_FAILURE = int(os.getenv("MAX_RETRIES_ON_FAILURE", 1))

# tweet_detail_scene.py
class TweetDetailScene:
    async def _scrape_replies(...):
        # ...
        try:
            # ...
            await first_reply_in_timeline.wait_for(timeout=20000) # 等待20秒
        except TimeoutError:
            return []
        # ...
        return replies

    async def scrape(...):
        # ...
        replies = await self._scrape_replies(...)
        scraped_data = {
            "reply_count": str(len(replies)), # 确保数据一致性
            # ...
            "replies": replies
        }
        return scraped_data

# production_crawler.py
async def main(args):
    # ...
    # --- [步骤 2/5] 抓取所有推文的详细数据 (包含重试逻辑) ---
    max_retries = getattr(config, 'MAX_RETRIES_ON_FAILURE', 1)
    for url in all_urls:
        for attempt in range(1 + max_retries):
            detailed_data = await crawler.scrape(...)
            if detailed_data and detailed_data.get("replies"):
                logger.info(f"    [抓取成功] 在第 {attempt + 1} 次尝试中成功...")
                break # 成功则退出重试
            if attempt < max_retries:
                logger.warning(f"    [抓取失败] 第 {attempt + 1} 次尝试未能...，即将重试...")
    # ...
```
