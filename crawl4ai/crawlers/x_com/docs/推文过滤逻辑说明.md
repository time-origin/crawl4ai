### **业务需求：过滤无回复推文**

**目标：**
过滤掉没有回复的推文，使其不被推送到 Kafka，并确保 `TASK_INIT` 消息中的 `total_tweets` 数量准确反映经过过滤后的有效推文数量。

**过滤条件：**
一条推文被认为是“无效”的，如果其 `tweet_data` 中 `replies` 字段为空列表（`[]`）或者不存在。我们将通过检查 `len(detailed_data.get("replies", [])) > 0` 来判断推文是否包含回复。

**修改文件：**
`crawl4ai/crawlers/x_com/production_crawler.py`

**修改位置和详细逻辑：**

在 `production_crawler.py` 文件的 `main` 异步函数中，我们将对推文的处理流程进行以下调整：

1.  **调整推文数据收集和过滤流程：**
    *   **原有流程：** 先扫描所有推文 URL (`all_urls`)，然后根据 `all_urls` 的数量计算 `total_tweets`，接着在后续循环中逐个 URL 获取详细数据并发送。
    *   **修改后流程：**
        *   首先，像以前一样扫描并收集所有潜在的推文 URL (`all_urls`)。
        *   然后，我们将引入一个新的列表 `all_filtered_tweets`。
        *   在获取到所有 URL 后，我们会立即循环遍历这些 URL，逐个调用 `crawler.scrape("tweet_detail", ...)` 来获取每条推文的详细数据 (`detailed_data`)。
        *   **过滤逻辑：** 在获取到 `detailed_data` 后，会立即应用过滤条件。如果 `detailed_data` 存在（即成功抓取到数据）并且其 `replies` 列表不为空（即 `len(detailed_data.get("replies", [])) > 0`），则将这条推文添加到 `all_filtered_tweets` 列表中。
        *   如果推文没有回复，将打印一条日志信息（使用 `logger.info`），并跳过该推文，不将其添加到 `all_filtered_tweets`。

2.  **更新 `total_tweets` 的计算：**
    *   `total_tweets` 的值将不再是原始 `all_urls` 的长度。
    *   修改后，`total_tweets` 将根据 `all_filtered_tweets` 列表的最终长度来计算。这样，发送到 `task_control_topic` 的 `TASK_INIT` 消息中报告的总推文数将准确反映经过过滤后的有效推文数量。

3.  **新增日志打印：**
    *   在计算出 `total_tweets` 之后，但在发送 `TASK_INIT` 消息之前，如果存在有效的推文，将打印出这些推文的 ID 和 URL，以便于调试和跟踪。日志描述语言为中文。

4.  **调整 Kafka/文件发送逻辑：**
    *   原有的 Kafka/文件发送循环是基于 `all_url_batches` 进行的，并且在循环内部再次调用 `crawler.scrape`。
    *   修改后，发送逻辑将直接迭代已经收集并过滤好的 `all_filtered_tweets` 列表。我们将把 `all_filtered_tweets` 分批（例如，每批 10 条）发送到 Kafka 或保存到文件。这意味着 `send_to_kafka` 或 `save_batch_to_file` 函数将直接接收到已经过滤过的、包含回复的推文数据批次。

**修改后的代码结构（伪代码示意）：**

```python
# ... (文件顶部导入和类定义保持不变) ...

async def main(args):
    # ... (初始化、认证检查、Kafka生产者设置等代码保持不变) ...

    try:
        # ... (Kafka或文件输出路径设置保持不变) ...

        async with MockBrowserManager() as mock_browser_manager:
            crawler = XProductionCrawler(config=config, browser_manager=mock_browser_manager)
            
            logger.info("--- Scanning Search Scene for Tweet URLs ---")
            scanner = await crawler.scrape("search", query=args.keyword, scroll_count=args.scan_scrolls)
            
            logger.info("--- Collecting all tweet URLs before processing ---")
            all_url_batches = [url_batch async for url_batch in scanner]
            all_urls = [url for batch in all_url_batches for url in batch]
            logger.info(f"--- Collected a total of {len(all_urls)} URLs for potential scraping ---")

            # [新增] 收集并过滤所有详细推文数据
            all_filtered_tweets = []
            should_fetch_replies = args.max_replies > 0 # 确保此变量已定义
            logger.info("--- Fetching detailed tweet data and applying reply filter ---")
            for url_batch in all_url_batches:
                for url in url_batch:
                    detailed_data = await crawler.scrape(
                        "tweet_detail",
                        url=url,
                        include_replies=should_fetch_replies,
                        max_replies=args.max_replies,
                        reply_scroll_count=args.reply_scrolls
                    )
                    # 过滤条件：如果推文有详细数据且包含回复，则保留
                    if detailed_data and detailed_data.get("replies") and len(detailed_data["replies"]) > 0:
                        all_filtered_tweets.append(detailed_data)
                    else:
                        logger.info(f"Skipping tweet {url} due to no replies.")

            # [修改] 根据过滤后的推文数量计算 total_tweets
            total_tweets = len(all_filtered_tweets)
            logger.info(f"--- After filtering, {total_tweets} tweets remain for processing ---")

            # [新增] 打印有效的推文列表
            if total_tweets > 0:
                logger.info("--- 有效推文列表 (仅显示ID和URL) ---")
                for tweet in all_filtered_tweets:
                    logger.info(f"  ID: {tweet.get('id', 'N/A')}, URL: {tweet.get('url', 'N/A')}")

            # [修改] 发送 TASK_INIT 消息到 Kafka (total_tweets 现在是过滤后的数量)
            if args.output_method == 'kafka' and total_tweets > 0:
                task_control_topic = getattr(config, 'KAFKA_TASK_TOPIC', 'task_control_topic')
                topic_ready = await ensure_topic_exists(bootstrap_servers=broker_url, topic_name=task_control_topic)
                if topic_ready:
                    logger.info(f"--- Sending TASK_INIT to topic '{task_control_topic}' ---")
                    await send_task_init_message(
                        producer=kafka_producer,
                        topic=task_control_topic,
                        original_task_id=args.original_task_id,
                        total_tweets=total_tweets
                    )

            # [修改] 迭代过滤后的推文列表进行发送
            # 可以根据需要调整批处理大小，这里使用一个示例值
            batch_size = 10 
            for i in range(0, total_tweets, batch_size):
                current_batch_tweets = all_filtered_tweets[i:i + batch_size]
                batch_num = (i // batch_size) + 1
                logger.info(f"\n--- PROCESSING BATCH {batch_num} ({len(current_batch_tweets)} filtered tweets) ---")

                if args.output_method == 'kafka':
                    await send_to_kafka(
                        producer=kafka_producer,
                        topic=kafka_topic,
                        data=current_batch_tweets, # 发送过滤后的批次
                        keyword=args.keyword,
                        original_task_id=args.original_task_id,
                        key_prefix=args.kafka_key_prefix
                    )
                else:
                    batch_file_path = run_output_dir / f"batch_{batch_num}.json"
                    await save_batch_to_file(data=current_batch_tweets, keyword=args.keyword, file_path=batch_file_path)

            logger.info("\n--- Streaming Workflow Complete ---")

    finally:
        # ... (Kafka生产者停止等清理工作保持不变) ...
```
