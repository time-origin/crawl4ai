# crawl4ai/crawlers/x_com/production_crawler.py

import asyncio
import importlib
import sys
import os
from pathlib import Path
from playwright.async_api import Page, async_playwright, expect, TimeoutError
import argparse
import re
import inspect
from datetime import datetime
from typing import List, Dict, Any

from playwright_stealth import stealth_async

# --- [FIX] Changed relative imports to absolute for PyInstaller compatibility ---
from crawl4ai.crawlers.x_com import config
from crawl4ai.crawlers.x_com.output_handler import save_batch_to_file
# [CHG] Import the new function for sending the task init message
from crawl4ai.crawlers.x_com.kafka_manager import send_to_kafka, ensure_topic_exists, send_task_init_message
from aiokafka import AIOKafkaProducer

# --- Logic for Bundled Execution ---
if getattr(sys, 'frozen', False) and hasattr(sys, '_MEIPASS'):
    browsers_path = os.path.join(sys._MEIPASS, 'ms-playwright')
    os.environ['PLAYWRIGHT_BROWSERS_PATH'] = browsers_path
    print(f"Running in bundled mode. Setting Playwright browsers path to: {browsers_path}")

class_logger = type("Logger", (), {"info": print, "warning": print, "error": print})
logger = class_logger()

# --- Dynamically determine the auth state path ---
auth_json_path_from_config = getattr(config, 'AUTH_JSON_PATH', None)

if auth_json_path_from_config:
    AUTH_STATE_PATH = Path(auth_json_path_from_config)
    print(f"Using authentication file from config: {AUTH_STATE_PATH}")
else:
    AUTH_STATE_PATH = Path(__file__).parent / "auth_state.json"
    print(f"Using default authentication file: {AUTH_STATE_PATH}")


def filter_and_log_valid_tweets(all_detailed_tweets: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    è¿‡æ»¤æ¨æ–‡åˆ—è¡¨ï¼Œåªä¿ç•™é‚£äº›å®é™…æŠ“å–åˆ°å›å¤å†…å®¹çš„æ¨æ–‡ï¼Œå¹¶è®°å½•æ—¥å¿—ã€‚
    
    Args:
        all_detailed_tweets: åŒ…å«æ‰€æœ‰å·²æŠ“å–æ¨æ–‡è¯¦æƒ…çš„åˆ—è¡¨ã€‚

    Returns:
        ä¸€ä¸ªåªåŒ…å«æœ‰æ•ˆæ¨æ–‡ï¼ˆå³ replies åˆ—è¡¨ä¸ä¸ºç©ºï¼‰çš„æ–°åˆ—è¡¨ã€‚
    """
    logger.info("--- [æ­¥éª¤ 3/5] å¼€å§‹è¿‡æ»¤æ¨æ–‡ï¼Œåªä¿ç•™åŒ…å«å®é™…å›å¤å†…å®¹çš„æ¨æ–‡ ---")
    
    valid_tweets = []
    for tweet_data in all_detailed_tweets:
        # è¿‡æ»¤æ¡ä»¶ï¼šæ¨æ–‡æ•°æ®å­˜åœ¨ï¼Œä¸”'replies'åˆ—è¡¨ä¸ä¸ºç©º
        if tweet_data and isinstance(tweet_data.get("replies"), list) and len(tweet_data["replies"]) > 0:
            valid_tweets.append(tweet_data)
        else:
            url_to_log = tweet_data.get('url', 'N/A') if tweet_data else 'N/A'
            logger.info(f"  [è¿‡æ»¤] æ¨æ–‡ {url_to_log} å·²è¢«è·³è¿‡ï¼ŒåŸå› ï¼šåœ¨æ‰€æœ‰å°è¯•åï¼Œä»æœªèƒ½æŠ“å–åˆ°ä»»ä½•å®é™…çš„å›å¤å†…å®¹ã€‚")

    logger.info(f"--- è¿‡æ»¤å®Œæˆã€‚åœ¨ {len(all_detailed_tweets)} æ¡æ¨æ–‡ä¸­ï¼Œå‘ç° {len(valid_tweets)} æ¡æœ‰æ•ˆæ¨æ–‡ã€‚ ---")

    # å¦‚æœå­˜åœ¨æœ‰æ•ˆæ¨æ–‡ï¼Œåˆ™æ‰“å°å…¶IDå’ŒURLåˆ—è¡¨
    if valid_tweets:
        logger.info("--- ä»¥ä¸‹æ˜¯æ‰€æœ‰æœ‰æ•ˆæ¨æ–‡çš„åˆ—è¡¨ (ID å’Œ URL) ---")
        for tweet in valid_tweets:
            logger.info(f"  [æœ‰æ•ˆ] ID: {tweet.get('id', 'N/A')}, URL: {tweet.get('url', 'N/A')}")
    
    return valid_tweets


class XProductionCrawler:
    def __init__(self, config, browser_manager):
        self.config = config
        self.browser_manager = browser_manager
        self.username = self.config.X_USERNAME
        self.password = self.config.X_PASSWORD

    async def login(self):
        # This method is intended for local execution to generate the auth file.
        if not self.username or not self.password:
            print("Login credentials (X_USERNAME, X_PASSWORD) not found in environment variables.")
            return
        page = await self.browser_manager.new_page(headless=False) # Login should not be headless
        if not page: return
        try:
            await page.goto("https://x.com/login", wait_until="domcontentloaded")
            
            try:
                await page.locator('input[name="text"]').fill(self.username)
            except TimeoutError as e:
                print("\n---")
                print("ğŸ›‘ DEBUGGING: Timeout occurred while finding the username input field.")
                page_html = await page.content()
                print("\n--- PAGE HTML CONTENT START ---")
                print(page_html)
                print("--- PAGE HTML CONTENT END ---\n")
                raise e

            next_button = (
                page.locator('[data-testid="ocfLoginNextLink"]')
                .or_(page.get_by_role("button", name="Next", exact=True))
                .or_(page.get_by_role("button", name="ä¸‹ä¸€æ­¥", exact=True))
            )
            await next_button.click()

            try:
                await page.locator('input[name="password"]').wait_for(timeout=10000)
            except TimeoutError:
                print("\nLogin failed: The password field did not appear.")
                raise
            
            await page.locator('input[name="password"]').fill(self.password)

            login_button = (
                page.locator('[data-testid="LoginForm_Login_Button"]')
                .or_(page.get_by_role("button", name="Log in", exact=True))
                .or_(page.get_by_role("button", name="ç™»å½•", exact=True))
            )
            await login_button.click()

            await expect(page.locator('[data-testid="primaryColumn"]')).to_be_visible(timeout=30000)
            await page.context.storage_state(path=str(AUTH_STATE_PATH))
        finally:
            await page.context.close()

    async def scrape(self, scene: str, **kwargs):
        page = await self.browser_manager.new_page(storage_state=str(AUTH_STATE_PATH))
        if not page: 
            async def empty_generator(): yield
            return empty_generator() if scene in ["search", "home"] else {}

        try:
            scene_module_name = f"crawl4ai.crawlers.x_com.scenes.{scene.lower()}_scene"
            scene_module = importlib.import_module(scene_module_name)
            scene_class_name = "".join(word.capitalize() for word in scene.split('_')) + "Scene"
            SceneClass = getattr(scene_module, scene_class_name)
            scene_instance = SceneClass()
        except (ImportError, AttributeError) as e:
            print(f"Error dynamically importing scene '{scene}': {e}")
            async def empty_generator(): yield
            return empty_generator() if scene in ["search", "home"] else {}

        result = scene_instance.scrape(page, **kwargs)
        if inspect.isasyncgen(result):
            async def page_managing_generator():
                try:
                    async for item in result:
                        yield item
                finally:
                    await page.context.close()
            return page_managing_generator()
        else:
            try:
                return await result
            finally:
                await page.context.close()

class MockBrowserManager:
    async def __aenter__(self):
        self.playwright = await async_playwright().start()
        return self
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        await self.playwright.stop()

    async def new_page(self, headless=True, storage_state=None):
        proxy_server = config.PROXY_SERVER
        
        browser_args = [
            '--no-sandbox',
            '--disable-setuid-sandbox',
            '--disable-dev-shm-usage',
            '--disable-gpu'
        ]

        launch_options = {
            "headless": headless, 
            "args": browser_args
        }

        if proxy_server:
            launch_options["proxy"] = {"server": proxy_server}
            print(f"Using proxy server: {proxy_server}")

        browser = await self.playwright.chromium.launch(**launch_options)
        context = await browser.new_context(storage_state=storage_state)
        page = await context.new_page()
        
        await stealth_async(page)

        return page

async def main(args):
    print("--- X.com ç”Ÿäº§ç¯å¢ƒçˆ¬è™«å¯åŠ¨ ---")

    if args.login:
        print("\nä»…æ‰§è¡Œç™»å½•æ¨¡å¼ï¼Œæ­£åœ¨å°è¯•ç”Ÿæˆè®¤è¯æ–‡ä»¶...")
        async with MockBrowserManager() as mock_browser_manager:
            crawler = XProductionCrawler(config=config, browser_manager=mock_browser_manager)
            await crawler.login()
        if AUTH_STATE_PATH.exists():
            print(f"ç™»å½•æˆåŠŸã€‚è®¤è¯æ–‡ä»¶å·²åœ¨ {AUTH_STATE_PATH} åˆ›å»º/æ›´æ–°ã€‚")
        else:
            print("ç™»å½•å¤±è´¥ï¼Œæœªèƒ½åˆ›å»ºè®¤è¯æ–‡ä»¶ã€‚")
        return

    # --- ä¸»æŠ“å–é€»è¾‘ ---
    kafka_producer = None
    run_output_dir = None
    broker_url = None
    kafka_topic = None

    try:
        print(f"\næ­£åœ¨æ£€æŸ¥è®¤è¯æ–‡ä»¶: '{AUTH_STATE_PATH}'")
        if not AUTH_STATE_PATH.exists():
            print(f"âŒ è®¤è¯æ–‡ä»¶æœªæ‰¾åˆ°: '{AUTH_STATE_PATH}'.")
            print("è¯·å…ˆåœ¨å¸¦æœ‰å›¾å½¢ç•Œé¢çš„æœºå™¨ä¸Šä½¿ç”¨ --login å‚æ•°è¿è¡Œæ­¤è„šæœ¬ä»¥ç”Ÿæˆè®¤è¯æ–‡ä»¶ï¼Œç„¶åå°†å…¶ä¸å¯æ‰§è¡Œæ–‡ä»¶ä¸€èµ·éƒ¨ç½²ã€‚")
            return
        else:
            print("âœ… è®¤è¯æ–‡ä»¶å·²æ‰¾åˆ°ï¼Œå¼€å§‹æŠ“å–æµç¨‹...")

        if args.output_method == 'kafka':
            broker_url = config.KAFKA_BOOTSTRAP_SERVERS or "localhost:9092"
            kafka_topic = config.KAFKA_TOPIC or "x_com_scraped_data"
            topic_ready = await ensure_topic_exists(bootstrap_servers=broker_url, topic_name=kafka_topic)
            if not topic_ready: return
            kafka_producer = AIOKafkaProducer(bootstrap_servers=broker_url)
            await kafka_producer.start()
        else:
            run_output_dir = Path(__file__).parent / "out" / "scraped" / f"{args.output_prefix}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            run_output_dir.mkdir(parents=True, exist_ok=True)
            print(f"æ–‡ä»¶æ‰¹æ¬¡è¾“å‡ºç›®å½•: {run_output_dir}")

        async with MockBrowserManager() as mock_browser_manager:
            crawler = XProductionCrawler(config=config, browser_manager=mock_browser_manager)
            
            # =============================================================================
            # æ­¥éª¤ 1/5: æ‰«ææ‰€æœ‰æ½œåœ¨çš„æ¨æ–‡ URL
            # =============================================================================
            logger.info("--- [æ­¥éª¤ 1/5] å¼€å§‹æ‰«ææœç´¢ç»“æœé¡µï¼Œè·å–æ‰€æœ‰æ¨æ–‡çš„URL ---")
            scanner = await crawler.scrape("search", query=args.keyword, scroll_count=args.scan_scrolls)
            all_url_batches = [url_batch async for url_batch in scanner]
            all_urls = [url for batch in all_url_batches for url in batch]
            logger.info(f"--- URLæ‰«æå®Œæˆï¼Œå…±å‘ç° {len(all_urls)} æ¡æ½œåœ¨çš„æ¨æ–‡URLã€‚ ---")

            # =============================================================================
            # æ­¥éª¤ 2/5: æŠ“å–æ‰€æœ‰æ¨æ–‡çš„è¯¦ç»†æ•°æ® (åŒ…å«é‡è¯•é€»è¾‘)
            # =============================================================================
            logger.info("--- [æ­¥éª¤ 2/5] å¼€å§‹ä¸ºæ‰€æœ‰URLæŠ“å–è¯¦ç»†çš„æ¨æ–‡æ•°æ®ï¼ˆåŒ…å«é‡è¯•ï¼‰ ---")
            all_detailed_tweets = []
            should_fetch_replies = args.max_replies > 0
            max_retries = getattr(config, 'MAX_RETRIES_ON_FAILURE', 1)

            for i, url in enumerate(all_urls, 1):
                logger.info(f"  æŠ“å–è¿›åº¦: {i}/{len(all_urls)} - URL: {url}")
                
                detailed_data = None
                # æ€»å°è¯•æ¬¡æ•° = 1 (é¦–æ¬¡) + max_retries
                for attempt in range(1 + max_retries):
                    detailed_data = await crawler.scrape(
                        "tweet_detail",
                        url=url,
                        include_replies=should_fetch_replies,
                        max_replies=args.max_replies,
                        reply_scroll_count=args.reply_scrolls
                    )

                    # æˆåŠŸæ¡ä»¶ï¼šæŠ“å–åˆ°äº†è‡³å°‘ä¸€æ¡å›å¤
                    if detailed_data and detailed_data.get("replies"):
                        logger.info(f"    [æŠ“å–æˆåŠŸ] åœ¨ç¬¬ {attempt + 1} æ¬¡å°è¯•ä¸­æˆåŠŸæŠ“å–åˆ° {len(detailed_data['replies'])} æ¡å›å¤ã€‚")
                        break # æˆåŠŸåˆ™é€€å‡ºé‡è¯•å¾ªç¯
                    
                    # å¤±è´¥ä¸”æœªåˆ°æœ€åä¸€æ¬¡å°è¯•ï¼Œåˆ™è®°å½•å¹¶å‡†å¤‡é‡è¯•
                    if attempt < max_retries:
                        logger.warning(f"    [æŠ“å–å¤±è´¥] ç¬¬ {attempt + 1} æ¬¡å°è¯•æœªèƒ½æŠ“å–åˆ°å›å¤ï¼Œå³å°†é‡è¯•...")
                        await asyncio.sleep(2) # é‡è¯•å‰çŸ­æš‚ç­‰å¾…
                
                # åœ¨æ‰€æœ‰å°è¯•ç»“æŸåï¼Œå¦‚æœä¾ç„¶æ²¡æœ‰å›å¤ï¼Œè®°å½•æœ€ç»ˆå¤±è´¥çŠ¶æ€
                if not (detailed_data and detailed_data.get("replies")):
                     logger.error(f"    [æœ€ç»ˆå¤±è´¥] åœ¨ {1 + max_retries} æ¬¡å°è¯•åï¼Œä»æœªèƒ½ä¸ºURL {url} æŠ“å–åˆ°ä»»ä½•å›å¤ã€‚")

                all_detailed_tweets.append(detailed_data)

            logger.info("--- æ‰€æœ‰URLçš„è¯¦ç»†æ•°æ®æŠ“å–å®Œæˆã€‚ ---")

            # =============================================================================
            # æ­¥éª¤ 3/5: è¿‡æ»¤æ¨æ–‡å¹¶è®°å½•æ—¥å¿—
            # =============================================================================
            all_filtered_tweets = filter_and_log_valid_tweets(all_detailed_tweets)

            # =============================================================================
            # æ­¥éª¤ 4/5: è®¡ç®—æœ€ç»ˆæ•°é‡å¹¶å‘é€ TASK_INIT æ¶ˆæ¯
            # =============================================================================
            total_tweets = len(all_filtered_tweets)
            logger.info(f"--- [æ­¥éª¤ 4/5] è®¡ç®—æœ‰æ•ˆæ¨æ–‡æ€»æ•°å®Œæˆï¼Œå…± {total_tweets} æ¡ã€‚ ---")
            if args.output_method == 'kafka' and total_tweets > 0:
                task_control_topic = getattr(config, 'KAFKA_TASK_TOPIC', 'task_control_topic')
                topic_ready = await ensure_topic_exists(bootstrap_servers=broker_url, topic_name=task_control_topic)
                if topic_ready:
                    logger.info(f"--- å‡†å¤‡å‘ä¸»é¢˜ '{task_control_topic}' å‘é€ TASK_INIT åˆå§‹åŒ–æ¶ˆæ¯ ---")
                    await send_task_init_message(
                        producer=kafka_producer,
                        topic=task_control_topic,
                        original_task_id=args.original_task_id,
                        total_tweets=total_tweets
                    )
            
            # =============================================================================
            # æ­¥éª¤ 5/5: æ‰¹å¤„ç†å¹¶åˆ†å‘æœ‰æ•ˆçš„æ¨æ–‡æ•°æ®
            # =============================================================================
            logger.info(f"--- [æ­¥éª¤ 5/5] å¼€å§‹æ‰¹å¤„ç†å¹¶åˆ†å‘ {total_tweets} æ¡æœ‰æ•ˆæ¨æ–‡ã€‚ ---")
            batch_size = 10 
            for i in range(0, total_tweets, batch_size):
                current_batch_tweets = all_filtered_tweets[i:i + batch_size]
                batch_num = (i // batch_size) + 1
                logger.info(f"\n--- æ­£åœ¨å¤„ç†ç¬¬ {batch_num} æ‰¹ï¼ŒåŒ…å« {len(current_batch_tweets)} æ¡æ¨æ–‡ ---")

                if args.output_method == 'kafka':
                    await send_to_kafka(
                        producer=kafka_producer,
                        topic=kafka_topic,
                        data=current_batch_tweets,
                        keyword=args.keyword,
                        original_task_id=args.original_task_id,
                        key_prefix=args.kafka_key_prefix
                    )
                else:
                    batch_file_path = run_output_dir / f"batch_{batch_num}.json"
                    await save_batch_to_file(data=current_batch_tweets, keyword=args.keyword, file_path=batch_file_path)

            logger.info("\n--- æ‰€æœ‰æ‰¹æ¬¡å¤„ç†å®Œæ¯•ï¼Œæµå¼å·¥ä½œæµå®Œæˆã€‚ ---")

    finally:
        if kafka_producer:
            print("æ­£åœ¨åœæ­¢ Kafka ç”Ÿäº§è€…...")
            await kafka_producer.stop()
            print("Kafka ç”Ÿäº§è€…å·²åœæ­¢ã€‚")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Scrape X.com for tweets in real-time.")
    
    # [ADD] New required argument for task identification
    task_group = parser.add_argument_group('Task Identification')
    task_group.add_argument("--original-task-id", type=int, required=True, help="The original task ID from the calling service.")

    login_group = parser.add_argument_group('Authentication')
    login_group.add_argument("--login", action="store_true", help="Perform the login process and exit.")
    
    scrape_group = parser.add_argument_group('Scraping Options')
    scrape_group.add_argument("--keyword", type=str, help="The search keyword to use.")
    scrape_group.add_argument("--scan-scrolls", type=int, default=1, help="Number of scrolls during URL scan (default: 1).")
    scrape_group.add_argument("--max-replies", type=int, default=0, help="Max replies to fetch. > 0 enables reply scraping.")
    scrape_group.add_argument("--reply-scrolls", type=int, default=5, help="Max scrolls to find replies (default: 5).")
    
    output_group = parser.add_argument_group('Output Options')
    output_group.add_argument("--output-method", type=str, choices=['file', 'kafka'], default='file', help="Choose output method: file or kafka.")
    output_group.add_argument("--output-prefix", type=str, default="x_com_scrape", help="Prefix for the run-specific output directory.")
    output_group.add_argument("--kafka-key-prefix", type=str, default="x.com", help="Prefix for the Kafka message key (default: x.com).")
    
    parsed_args = parser.parse_args()
    if not parsed_args.login and not parsed_args.keyword:
        parser.error("the --keyword argument is required when not performing --login.")
    
    asyncio.run(main(parsed_args))
